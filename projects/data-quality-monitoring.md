---
description: Coming Soon
---

# How Owl DQ and Schema Learned makes your Data Lake better.

Can you take action on data developed in a Data Lake, real-time or not, without first performing a DQ check? Can you trigger action before checking the “GDPR Remove” list?  Step 1 will always be a Data Quality check.  OwlDQ with Schema Learned can perform an owl check on 100 things in a stream at the source, without requiring data to ever be moved.    Churn, credit check, AML, infosec checks developed in the Data Lake could be added as part of Owlcheck on the streaming data.

* **Data and Privacy in Place.**  Data does not have to move for a DQ OwlCheck.  The latency saved and the added hybrid flexibility serves many new use cases that were not possible before and it removes any unnecessary consolidation for the sake of simply consolidation.



* **Streaming.**  Rules learned by Owl or in the Data Lake can be applied back to the source in the stream.  The first step for any ML applied real-time recommendation is a DQ check.  The DQ check has to happen at the source to enable near real-time ML.



* **Self-Service and DQ push down fix.**  A push-down fix \(recommendation engine\) to anything flagged at the source.  The best time to fix DQ is when and where the problem started.  This enables tighter integration with Data Governance tools since DQ is maintained at the source once, not downstream where corruption beyond just the data can occur. 



* **Multi-cloud/On-prem/Hybrid.**  OwlDQ can scan/alert/report at the source or can operate natively on the target Data Lake with products such as Databricks Delta in Azure or Snowflake on AWS, or Qubole on GCP.  



  * **Speed Migrations/Enable Replications.** Batch collection with subsequent excel compare is very problematic.  Instead, rules are generated by the data itself, and anomalies are triggered on the fly at the source system.  This type of schema learned approach will speed migration and enable replications with much less overhead.

 

* **DQ Dashboards.**  Many DQ problems result from improper or too slow observation of Schema creep.  What is not caught by handmade visual inspection or the accompanying man-made rule can only be flagged by AI observation.  Conversely, what does get flagged should be easily triaged and immediately fixed with the aid of AI.  The most important metric for a DQ Dashboard is time to fix, not simply overall DQ score.



* **A Special Case of a Data Aggregator** 



  * **Greatly reduce Support Costs.**  Apply AI-generated rules to maintain consistency across all accounts, rather than apply hand-made rules per every account.  You are not managing 100+ variations of rules, but a consistent set that is learned from all. 

 

  * **Capitalize on Real-time.**  Batch collection with excel compare will never support real-time.  Instead, rules are generated by the data itself, and anomalies are triggered on the fly at the source system.

  * **Rapid Onboarding.**  Universal and already tested Scans are applied quickly.



  * **Improve Customer Satisfaction.** Monitor the real-time speed of the DQ pushdown fix, not just the overall DQ score over time.



  * **Improved SLAs.**  When DQ is fixed immediately, all SLAs can be improved not just DQ SLAs.  

{% embed url="https://www.youtube.com/watch?v=Lu2I7n\_nRlc&t=6s" %}

## Data Aggregator Video

Aggregation from 100s of locations.  The dashboard for what is within spec based on AI observation – not handmade rules.  

Then push-down fix.  The rules are created and then immediately applied via self-service.  The problem is immediately identified and a fix \(recommendation engine\) is applied.  

The Value of this for both companies.  The DQ problem never corrupts the whole.  The longer the bad quality exists the bigger problem it can create.  

